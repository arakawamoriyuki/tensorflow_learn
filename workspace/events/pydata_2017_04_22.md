# Pydata okinawa 2017-04-22

- sonnet coreに近い python2系のみ??

深層学習フレームワーク
tensor
keras
caffe
pytouch

[nips2017](https://nips.cc/)
chatbotの国際ガチコンペ

## TensorFlow Core入門

1. 線形回帰
2. 多層パーセプトロン
3. CNN


tensorflowはディープラーニングの為のライブラリではない！
もっと広義な機械学習ライブラリ
kerasやscikit-learn、sonnetとは違う

https://www.tensorflow.org/get_started/get_started
https://www.tensorflow.org/install/
https://www.tensorflow.org/api_docs/python/
https://www.tensorflow.org/api_docs/python/tf/train


[Welcome to TensorFlow!](http://web.stanford.edu/class/cs20si/lectures/slides_01.pdf)


### jupiter

shift+enter
tab保管
kernel reset

### 計算グラフ

ioが遅い 全メモリ 最適化されたノード割り振り
cpuだろうがgpuだろうがデバイスに最適化された計算

### 計算グラフの構築

出力は計算グラフ(Tensorクラス)

### 計算グラフの実行

作成した計算ノードをSessionで計算する
遅延評価的な???
(C++で処理される)

InteractiveSessionはデフォルトグラフに入る = tensorboradの???

t.eval()は一個
sess.run([a, b, c])は複数

### TensorFlowのセッション: tf.Session()

with tf.Session() as sess:

withで開いたsessionは抜けると自動でcloseされる。

### TensorFlowのインタラクティブセッション: tf.InteractiveSession()

sess.close()で閉じなければならない


feed_dictで定義した変数を変更する事ができる
sess.run([node1, node2, node3], feed_dict={node1:10.0}


### 計算グラフの可視化

#### console(text)

tf.get_default_graph().as_graph_def()でテキストで可視化

#### tensorboard

writer = tf.summary.FileWriter('./graphs01', sess.graph)  # イベントファイルを保存
writer.close()

tensorboard --logdir="./graphs01" --port 6006
localhost:6006


### 外部入力

tf.placeholder

a = tf.placeholder(tf.float32, shape=[1], name="a_value")
b = tf.placeholder(tf.float32, shape=[1], name="b_value")
adder_node = tf.add(a, b)

### 初期化

tf.valiableは初期化が必要

init = tf.global_variables_initializer()  # 変数を初期化する処理を計算グラフに追加
sess.run(init) # 初期化を実行

### 最適化

```
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
all_loss = []
all_W = []
all_b = []

sess.run(init)

curr_loss, curr_W, curr_b = sess.run([loss, W, b],
                                     feed_dict={x: [1, 2, 3, 4], y: [0, -1, -2, -3]})

all_loss.append(curr_loss)
all_W.append(curr_W)
all_b.append(curr_b)

for i in range(500):

    _, curr_loss , curr_W, curr_b= sess.run([train, loss, W, b],
                            feed_dict={x: [1, 2, 3, 4], y: [0, -1, -2, -3]})

    all_loss.append(curr_loss)
    all_W.append(curr_W)
    all_b.append(curr_b)

    fig, ax = plt.subplots(2,1,figsize=(10, 10))

    ax[0].plot(all_loss)
    ax[0].set_ylabel('Loss')


    ax[1].plot(all_W, label='W')
    ax[1].plot(all_b, label='b')
    ax[1].set_ylabel('Parameter values')
    ax[1].legend()
```


### 結果

b = 1
W = -1

式的には1次関数！
linear regression

```
y = Wx + b
```

#
